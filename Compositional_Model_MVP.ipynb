{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7WRR0o/V5HvHFn7jPg7Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/DisCoFuzz/blob/main/Compositional_Model_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Config"
      ],
      "metadata": {
        "id": "DDepQNr4daw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import spacy\n",
        "from typing import Optional, Iterable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOG95siGdcsY",
        "outputId": "8d675f5b-c9fc-4e8b-aac0-f853575f4b4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LemmaVectorizer:\n",
        "    def __init__(self, jit_vectorization = False):\n",
        "        self.embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.keyed_vectors = None\n",
        "        if not jit_vectorization:\n",
        "          self.keyed_vectors = self.build_wordnet_lemma_embeddings()\n",
        "\n",
        "    def build_wordnet_lemma_embeddings(\n",
        "        self,\n",
        "        batch_size: int = 64\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Loads all WordNet lemmas, embeds them using a SentenceTransformer model,\n",
        "        and returns a dictionary mapping lemma → embedding vector.\n",
        "\n",
        "        Args:\n",
        "            batch_size: Batch size for model.encode().\n",
        "\n",
        "        Returns:\n",
        "            dict[str, np.ndarray]: mapping from lemma string to embedding vector.\n",
        "        \"\"\"\n",
        "        # Collect all lemmas (set removes duplicates)\n",
        "        lemma_set = set()\n",
        "        for syn in wn.all_synsets():\n",
        "            for lemma in syn.lemmas():\n",
        "                lemma_set.add(lemma.name().replace(\"_\", \" \"))  # normalize underscore → space\n",
        "\n",
        "        lemma_list = sorted(lemma_set)\n",
        "\n",
        "        # Encode lemmas in batches\n",
        "        embeddings = self.embedding_model.encode(\n",
        "            lemma_list,\n",
        "            batch_size=batch_size,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Build dictionary\n",
        "        lemma_to_vec = {lemma: emb for lemma, emb in zip(lemma_list, embeddings)}\n",
        "\n",
        "        return lemma_to_vec\n",
        "\n",
        "    def __call__(self, X: str) -> np.ndarray:\n",
        "        if self.keyed_vectors:\n",
        "            v = self.keyed_vectors.get(X)#.lemma_.lower())\n",
        "            if v is not None:\n",
        "                return np.asarray(v, dtype=float)\n",
        "            # If a token is not in keyed_vectors, it will simply not contribute an embedding\n",
        "            # unless other parts of the composition strategy can provide one.\n",
        "            return None\n",
        "\n",
        "        if not self.keyed_vectors:\n",
        "            return self.embedding_model.encode([X])"
      ],
      "metadata": {
        "id": "KpgUdf7wZYKt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformer model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "lemma_vectorizer = LemmaVectorizer(True)"
      ],
      "metadata": {
        "id": "rjzkj1U3dmMs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "rAmGzGhevh0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Uw_u3kjWLGy"
      },
      "outputs": [],
      "source": [
        "def _is_ignored(token):\n",
        "    return token.is_punct or token.pos_ == \"DET\"\n",
        "\n",
        "def node_embedding(token, cache):\n",
        "    \"\"\"Recursively compute embedding for the branch rooted at token.\"\"\"\n",
        "    if token.i in cache:\n",
        "        return cache[token.i], cache\n",
        "\n",
        "    # get pairwise embedding means, then get average of those\n",
        "    if token.pos_ == \"VERB\":\n",
        "        verb_vec = lemma_vectorizer(token.lemma_.lower())\n",
        "        pair_means = []\n",
        "\n",
        "        for child in token.children:\n",
        "            child_emb, cache = node_embedding(child, cache)\n",
        "            if verb_vec is not None and child_emb is not None:\n",
        "                pair_means.append((verb_vec + child_emb) / 2)\n",
        "\n",
        "        if len(pair_means) > 0:\n",
        "            stacked = np.vstack(pair_means)\n",
        "            result = stacked.mean(axis=0)\n",
        "            cache[token.i] = result\n",
        "            return result, cache\n",
        "        # If no usable children, fall back to general logic below.\n",
        "\n",
        "    # get mean of embeddings\n",
        "    collected = []\n",
        "\n",
        "    # include token vector unless ignored\n",
        "    if not _is_ignored(token):\n",
        "        vec = lemma_vectorizer(token.lemma_.lower())\n",
        "        if vec is not None:\n",
        "            collected.append(vec)\n",
        "\n",
        "    # include all children's embeddings\n",
        "    for child in token.children:\n",
        "        emb, cache = node_embedding(child, cache)\n",
        "        if emb is not None:\n",
        "            collected.append(emb)\n",
        "\n",
        "    if len(collected) == 0:\n",
        "        result = np.zeros(384) # Ensure a 384-dim zero vector if no embeddings are found\n",
        "    else:\n",
        "        result = np.vstack(collected).mean(axis=0)\n",
        "\n",
        "    cache[token.i] = result\n",
        "    return result, cache\n",
        "\n",
        "\n",
        "def embed_doc_by_dependency(\n",
        "    text: str,\n",
        "):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    cache = {}\n",
        "\n",
        "    # Aggregate sentence root embeddings\n",
        "    root_embeddings = []\n",
        "    for sent in doc.sents:\n",
        "        root = sent.root\n",
        "        emb, cache = node_embedding(root, cache)\n",
        "        if emb is not None:\n",
        "            root_embeddings.append(emb)\n",
        "\n",
        "    if len(root_embeddings) == 0:\n",
        "        return np.zeros(384)\n",
        "\n",
        "    return np.vstack(root_embeddings).mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test embedding composition"
      ],
      "metadata": {
        "id": "TcbBgkRXdfww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = embed_doc_by_dependency(\"the quick brown fox jumps over the lazy dog.\")\n",
        "v2 = embed_doc_by_dependency(\"the quick brown dog jumps over the lazy fox.\")\n",
        "v3 = embed_doc_by_dependency(\"alice loves bob.\")"
      ],
      "metadata": {
        "id": "m1qM5CqjWTNc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(f\"similarity of similar sentences: {cosine_similarity([v1], [v2])}\")\n",
        "print(f\"similarity of dissimilar sentences: {cosine_similarity([v1], [v3])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGJwvxF8XHOz",
        "outputId": "8e62a988-5267-4bd4-8c96-3ffc637f8bea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity of similar sentences: [[0.99968085]]\n",
            "similarity of dissimilar sentences: [[0.45042074]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cb_ZH7Ibrm1a"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}